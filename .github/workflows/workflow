name: Evaluate Transcription Script

on:
  push:
    branches: [ main ] # Or 'master' depending on your default branch
    paths:
      - 'transcription.py' # Only run if the script changes
  workflow_dispatch: # Allows manual triggering

jobs:
  evaluate:
    runs-on: ubuntu-latest
    timeout-minutes: 10 # Prevent runaway scripts/API calls

    env:
      # The specific URL to be used for evaluation
      TARGET_IMAGE_URL: "http://aalt.law.uh.edu/CP25%281%29/CP25%281%29Nhants179/IMG_0275.JPG"
      # IMPORTANT: Maps the GitHub Secret to the env var the student script expects
      LLM_API_KEY: ${{ secrets.LLM_API_KEY }}

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.10' # Choose a suitable Python version

    - name: Install Dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests Pillow python-dotenv openai google-generativeai beautifulsoup4 opencv-python-headless jiwer
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
      continue-on-error: false

    - name: Check for required file and function definition
      id: check_files
      run: |
        if [ ! -f transcription.py ]; then
          echo "ERROR: transcription.py not found!"
          exit 1
        fi
        # Check for the function definition pattern (accepts with or without type hints)
        if ! grep -q "def get_transcription(" transcription.py; then
          echo "ERROR: Function definition 'def get_transcription(...)' not found in transcription.py!"
          exit 1
        fi
        echo "Required file and function definition pattern found."

    - name: Run Evaluation Script
      id: run_eval
      run: |
        # Create a helper script to run student code safely and calculate CER
        cat << 'EOF' > run_evaluation.py
        import os
        import sys
        import importlib
        import traceback
        from jiwer import compute_measures

        STUDENT_SCRIPT = "transcription"
        TARGET_FUNCTION = "get_transcription"
        GROUND_TRUTH_FILE = "ground_truth.txt"
        API_KEY_NAME = 'LLM_API_KEY' # Standardized API key env var name
        URL_ENV_VAR = 'TARGET_IMAGE_URL' # Env var holding the URL for evaluation

        def calculate_score(cer):
            """Calculates score based on CER using linear interpolation."""
            if cer < 0: cer = 0
            if cer > 1: cer = 1

            if cer == 0.0: return 100.0
            elif cer <= 0.50: return 100.0 + (cer - 0.0) * (50.0 - 100.0) / (0.50 - 0.0)
            elif cer <= 0.95: return 50.0 + (cer - 0.50) * (5.0 - 50.0) / (0.95 - 0.50)
            else: return 5.0

        print(f"--- Loading Ground Truth from {GROUND_TRUTH_FILE} ---")
        try:
            with open(GROUND_TRUTH_FILE, 'r', encoding='utf-8') as f:
                ground_truth = f.read().strip()
            if not ground_truth:
                print(f"ERROR: Ground truth file '{GROUND_TRUTH_FILE}' is empty.")
                sys.exit(1)
            print("Ground truth loaded successfully.")
        except FileNotFoundError:
            print(f"ERROR: Ground truth file '{GROUND_TRUTH_FILE}' not found.")
            sys.exit(1)
        except Exception as e:
            print(f"ERROR: Failed to read ground truth file: {e}")
            sys.exit(1)

        # --- Get Target URL for Evaluation ---
        target_image_url = os.getenv(URL_ENV_VAR)
        if not target_image_url:
            print(f"ERROR: Environment variable {URL_ENV_VAR} is not set in the workflow.")
            sys.exit(1)
        print(f"Target Image URL for evaluation: {target_image_url}")

        print(f"--- Importing {TARGET_FUNCTION} from {STUDENT_SCRIPT}.py ---")
        student_transcription = None
        try:
            sys.path.insert(0, os.getcwd())
            module = importlib.import_module(STUDENT_SCRIPT)
            if not hasattr(module, TARGET_FUNCTION):
                print(f"ERROR: Function '{TARGET_FUNCTION}' not found in {STUDENT_SCRIPT}.py.")
                sys.exit(1)

            get_transcription_func = getattr(module, TARGET_FUNCTION)

            print(f"--- Executing {STUDENT_SCRIPT}.{TARGET_FUNCTION}(image_url='{target_image_url}') ---")
            # Check for API key presence
            if not os.getenv(API_KEY_NAME):
                 print(f"ERROR: Environment variable {API_KEY_NAME} is not set.")
                 print("Ensure the LLM_API_KEY secret is configured in GitHub settings.")
                 sys.exit(1)

            # *** Call the student function WITH the target URL ***
            student_transcription = get_transcription_func(image_url=target_image_url)

            if not isinstance(student_transcription, str):
                print(f"ERROR: The '{TARGET_FUNCTION}' function did not return a string.")
                print(f"Returned type: {type(student_transcription)}")
                sys.exit(1)

            student_transcription = student_transcription.strip()
            print("Student script executed successfully.")

        except ImportError:
            print(f"ERROR: Could not import {STUDENT_SCRIPT}.py. Check for syntax errors.")
            traceback.print_exc()
            sys.exit(1)
        except Exception as e:
            print(f"ERROR: An error occurred while running the student script '{STUDENT_SCRIPT}.py'.")
            print("--- Error Traceback ---")
            traceback.print_exc()
            print("--- End Traceback ---")
            student_transcription = None # Indicate failure

        # --- Evaluation ---
        print("\n--- Evaluation ---")
        cer = 1.0 # Default to 100% CER
        score = 0.0 # Default to 0 points for non-functional script

        if student_transcription is None:
            print("Evaluation skipped due to script execution error.")
            # Score remains 0
        elif not student_transcription:
             print("WARNING: Student script returned an empty transcription.")
             cer = 1.0
             score = calculate_score(cer) # Should be 5 points
        else:
            try:
                measures = compute_measures(ground_truth, student_transcription)
                cer = measures['wer'] # jiwer calculates CER by default here
                print(f"Character Error Rate (CER): {cer:.4f} ({cer*100:.2f}%)")
                score = calculate_score(cer)
            except Exception as e:
                print(f"ERROR: Could not calculate CER. Error: {e}")
                traceback.print_exc()
                cer = 1.0 # Assign 100% CER on calculation error
                score = 0.0 # Assign 0 points if CER calculation fails

        print(f"Calculated Score: {score:.2f} / 100.0 Extra Credit Points")

        # Output results for summary step
        print(f"::set-output name=cer::{cer:.4f}")
        print(f"::set-output name=score::{score:.2f}")
        print(f"::set-output name=student_transcription_preview::{student_transcription[:100] if student_transcription else 'N/A'}...")

        EOF

        # Execute the helper script
        python run_evaluation.py

    - name: Display Results Summary
      if: always() # Always run this step
      run: |
        echo "--- Evaluation Summary ---"
        echo "Target Image URL: ${{ env.TARGET_IMAGE_URL }}"
        echo "Script Status: ${{ job.status }}"
        if [ "${{ steps.run_eval.outcome }}" == "success" ]; then
          echo "CER: ${{ steps.run_eval.outputs.cer }} (${{ steps.run_eval.outputs.cer * 100 }}%)"
          echo "Score: ${{ steps.run_eval.outputs.score }} / 100.0"
          echo "Student Transcription Preview: ${{ steps.run_eval.outputs.student_transcription_preview }}"
        elif [ "${{ steps.check_files.outcome }}" != "success" ]; then
           echo "Evaluation failed: Required file or function definition missing."
           echo "Score: 0 / 100.0"
        else
           echo "Evaluation failed during script execution or CER calculation."
           echo "Score: 0 / 100.0" # Non-functional script gets 0 points
           echo "(Check logs above for specific errors)"
        fi
        echo "-------------------------"
